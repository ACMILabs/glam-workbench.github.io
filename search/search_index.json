{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the wonderful world of GLAM data! \u00b6 Here you\u2019ll find a collection of tools and examples to help you work with data from galleries, libraries, archives, and museums (the GLAM sector), focusing on Australia and New Zealand. What is GLAM data? \u00b6 When we talk about GLAM data we\u2019re usually referring to the collections held by cultural institutions \u2013 books, manuscripts, photographs, objects, and much more. We\u2019re used to exploring these collections through online search interfaces or finding aids, but sometimes we want to do more \u2013 instead of a list of search results on a web page, we want access to the underlying collection data for analysis, enrichment, or visualisation. We want collections as data . This GLAM Workbench shows you how to create your own research datasets from a variety of GLAM collections. In some cases cultural institutions provide direct access to collection data through APIs (Application Programming Interfaces) or data downloads. In other cases we have to find ways of extracting data from web interfaces \u2013 a process known as screen-scraping. Here you\u2019ll find examples of all these approaches, as well as links to a number of pre-harvested datasets. What can I do with GLAM data? \u00b6 Ask different types of questions! Shift scales Find patterns Extract features Make connections This GLAM Workbench demonstrates a variety of tools and techniques that you can use to explore your data. Do I need to be able to code? \u00b6 No, you can use the Jupyter notebooks within the workbench without any coding experience \u2013 just edit and click where indicated. But every time you do edit one of the notebooks, you are coding. The notebooks provide an opportunity to gain confidence and experiment. They might not turn you into a coder, but they will show you how to do useful things with code. What is Jupyter? \u00b6 ~~More detail required~~ Where do I start? \u00b6 ~~More detail required~~ Other GLAM related notebooks \u00b6 Awesome Jupyter GLAM Getting started with ODate","title":"Home"},{"location":"#welcome-to-the-wonderful-world-of-glam-data","text":"Here you\u2019ll find a collection of tools and examples to help you work with data from galleries, libraries, archives, and museums (the GLAM sector), focusing on Australia and New Zealand.","title":"Welcome to the wonderful world of GLAM data!"},{"location":"#what-is-glam-data","text":"When we talk about GLAM data we\u2019re usually referring to the collections held by cultural institutions \u2013 books, manuscripts, photographs, objects, and much more. We\u2019re used to exploring these collections through online search interfaces or finding aids, but sometimes we want to do more \u2013 instead of a list of search results on a web page, we want access to the underlying collection data for analysis, enrichment, or visualisation. We want collections as data . This GLAM Workbench shows you how to create your own research datasets from a variety of GLAM collections. In some cases cultural institutions provide direct access to collection data through APIs (Application Programming Interfaces) or data downloads. In other cases we have to find ways of extracting data from web interfaces \u2013 a process known as screen-scraping. Here you\u2019ll find examples of all these approaches, as well as links to a number of pre-harvested datasets.","title":"What is GLAM data?"},{"location":"#what-can-i-do-with-glam-data","text":"Ask different types of questions! Shift scales Find patterns Extract features Make connections This GLAM Workbench demonstrates a variety of tools and techniques that you can use to explore your data.","title":"What can I do with GLAM data?"},{"location":"#do-i-need-to-be-able-to-code","text":"No, you can use the Jupyter notebooks within the workbench without any coding experience \u2013 just edit and click where indicated. But every time you do edit one of the notebooks, you are coding. The notebooks provide an opportunity to gain confidence and experiment. They might not turn you into a coder, but they will show you how to do useful things with code.","title":"Do I need to be able to code?"},{"location":"#what-is-jupyter","text":"~~More detail required~~","title":"What is Jupyter?"},{"location":"#where-do-i-start","text":"~~More detail required~~","title":"Where do I start?"},{"location":"#other-glam-related-notebooks","text":"Awesome Jupyter GLAM Getting started with ODate","title":"Other GLAM related notebooks"},{"location":"about/","text":"This is a collection of Jupyter notebooks to help you explore and use data from GLAM institutions. It's aimed at researchers in the humanities, but will include examples and tutorials of more general interest. Over the past decade I've created and shared a wide variety of digital tools, examples, tutorials, and datasets. Some like QueryPic and TroveHarvester are fairly polished and well documented. Others are just fragments of code. All of them are intended to support research into our rich cultural collections. But even though something like the TroveHarvester is pretty easy to use, it does require a bit of set-up, and I've been very aware that this can be a barrier to people starting their explorations. I created the dhistory site many years ago to provide the foundation for a digital workbench, but I couldn't quite achieve what I wanted \u2014 tools that were easy to use and required minimal setup, but also tools that exposed their own workings, that inspired novice users to question and to tinker. So here we are. My plan is to use Jupyter , GitHub, and Binder to bring together all those tools, examples, tutorials, and datasets in a way that supports people's explorations through digital GLAM collections. I'm really excited, for example, that I can create a notebook that provides a deconstructed (or perhaps see-through) version of QueryPic \u2014 that enables you to build, step by step, the same sort of visualisations, while learning about how it works. And at the end you can download the results as a CSV for further analysis. I love the way that Jupyter notebooks combine learning with real, live, digital tools and methods. You don't have to read a tutorial then go away and try to follow the instructions on your own. It's all together. Live code. Real research. Active learning. Like most of my projects this is in itself an experiment. I'm still learning what's possible and what works. But I'm hopeful. If you think this project is worthwhile, you might like to support me on Patreon .","title":"Some background"},{"location":"archway/","text":"Archway \u00b6 Archway is the collections database of Archives New Zealand and provides rich, contextual information about records, series, agencies, and functions. Unfortunately Archway doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. Tools, tips, and examples \u00b6 Harvesting a records search This notebook includes code that will enable you to harvest individual record details from a search in Archway. There's a limit of 1,000 results returned for any search, so if you want to harvest more records than this, you'll need to break your search up into chunks of less than 1,000. Harvesting functions Functions provide an alternative way of finding relevant series and records \u2014 zooming out from the records to focus on the government activities you're interested in. Functions also provide an interesting data point to analyse and visualise. This notebook lets you download the functions used by Archway as a dataset. Useful apps \u00b6 Who's responsible? Archives New Zealand divides government activities up into 303 functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from Archway to create a a simple visualisation of the agencies responsible for a selected function.","title":"Archway"},{"location":"archway/#archway","text":"Archway is the collections database of Archives New Zealand and provides rich, contextual information about records, series, agencies, and functions. Unfortunately Archway doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping.","title":"Archway"},{"location":"archway/#tools-tips-and-examples","text":"Harvesting a records search This notebook includes code that will enable you to harvest individual record details from a search in Archway. There's a limit of 1,000 results returned for any search, so if you want to harvest more records than this, you'll need to break your search up into chunks of less than 1,000. Harvesting functions Functions provide an alternative way of finding relevant series and records \u2014 zooming out from the records to focus on the government activities you're interested in. Functions also provide an interesting data point to analyse and visualise. This notebook lets you download the functions used by Archway as a dataset.","title":"Tools, tips, and examples"},{"location":"archway/#useful-apps","text":"Who's responsible? Archives New Zealand divides government activities up into 303 functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from Archway to create a a simple visualisation of the agencies responsible for a selected function.","title":"Useful apps"},{"location":"digitalnz/","text":"DigitalNZ aggregates collections from across New Zealand and makes the aggregated metadata available through an API . You'll need an API key to work with DigitalNZ data. Tips, tools, and examples \u00b6 Getting some top-level data from the DigitalNZ API This notebook pokes around at the top-level of DigitalNZ, mainly using facets to generate some collection overviews and summaries. Find results by country in DigitalNZ Many items in DigtalNZ include location information. This can include a country, but as far as I can see there's no direct way to search for results relating to a particular country using the API. You can, however, search for geocoded locations using bounding boxes. This notebook shows how you can use this to search for countries. Visualise a search in Papers Past Start with some keywords you want to search for in Papers Past , then create a simple visualisation showing the distribution over time and by newspaper. Harvest data from Papers Past This notebooks lets you harvest large amounts of data for Papers Past (via DigitalNZ) for further analysis. It saves the results as a CSV file that you can open in any spreadsheet program. It currently includes the OCRd text of all the newspaper articles.","title":"DigitalNZ"},{"location":"digitalnz/#tips-tools-and-examples","text":"Getting some top-level data from the DigitalNZ API This notebook pokes around at the top-level of DigitalNZ, mainly using facets to generate some collection overviews and summaries. Find results by country in DigitalNZ Many items in DigtalNZ include location information. This can include a country, but as far as I can see there's no direct way to search for results relating to a particular country using the API. You can, however, search for geocoded locations using bounding boxes. This notebook shows how you can use this to search for countries. Visualise a search in Papers Past Start with some keywords you want to search for in Papers Past , then create a simple visualisation showing the distribution over time and by newspaper. Harvest data from Papers Past This notebooks lets you harvest large amounts of data for Papers Past (via DigitalNZ) for further analysis. It saves the results as a CSV file that you can open in any spreadsheet program. It currently includes the OCRd text of all the newspaper articles.","title":"Tips, tools, and examples"},{"location":"getting-started/","text":"Using Jupyter notebooks \u00b6 Some general tips: Code cells have boxes around them. To run a code cell click on the cell and then hit Shift+Enter . The Shift+Enter combo will also move you to the next cell, so it\u2019s a quick way to work through the notebook. While a cell is running a * appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number. In most cases you\u2019ll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones. To edit a code cell, just click on it and type stuff. Remember to run the cell once you\u2019ve finished editing. Viewing notebooks on NBViewer \u00b6 The links on the title of each notebook will open it in NBViewer . NBViewer takes a notebook from a GitHub repository and renders it in a nice, readable way. However, this is a static view of the notebook. If you want to run the code or edit the notebook, you need to run it on Binder. Either click one of the buttons described below or the Binder icon in NBViewer's top menu bar. Running notebooks live on Binder \u00b6 Each collection of notebooks includes a button. When you click on the button, the Binder service opens the notebooks within a customised computing environment. This can take a little while \u2014 just be patient. Once Binder is ready, you'll be able to use all the notebooks live within your web browser. However, if you make any changes or download any data, Binder won't save them for you. You'll have to make sure you download any files you want to keep. In many cases the notebooks themselves will generate download links to make it easy for you to save your results. Binder sessions will also stop responding after after a period of inactivity \u2014 just start a new session. Running notebooks in \u2018app mode\u2019 \u00b6 Some of the workbench repositories make use of the appmode extension for Jupyter notebooks. When you open a notebook in app mode, all the code cells are hidden and are run automatically as the notebook loads. This means you can make a notebook available with a nice clean interface for those who might be a little intimidated by a page full of code. But the code is still there. To view the underlying notebook, just click on the 'Edit App' button at the top of the page. There are two ways to open a notebook in app mode. If you're in the normal notebook view you should see an appmode button in the menu bar, just click it. To make things easier, I've included buttons under each app \u2014 when you click these buttons, the notebooks open on Binder in app mode. No extra clicks required. Running notebooks on your own computer \u00b6 If you have Jupyter running on your own computer you can just clone, fork, or download the repository from GitHub. The link to the repository on GitHub is in the top navigation bar. Once you have a local copy (preferably running in a virtual environment), you can install the siftware you need using the requirements.txt file. ~~More detail required~~","title":"Getting started"},{"location":"getting-started/#using-jupyter-notebooks","text":"Some general tips: Code cells have boxes around them. To run a code cell click on the cell and then hit Shift+Enter . The Shift+Enter combo will also move you to the next cell, so it\u2019s a quick way to work through the notebook. While a cell is running a * appears in the square brackets next to the cell. Once the cell has finished running the asterix will be replaced with a number. In most cases you\u2019ll want to start from the top of notebook and work your way down running each cell in turn. Later cells might depend on the results of earlier ones. To edit a code cell, just click on it and type stuff. Remember to run the cell once you\u2019ve finished editing.","title":"Using Jupyter notebooks"},{"location":"getting-started/#viewing-notebooks-on-nbviewer","text":"The links on the title of each notebook will open it in NBViewer . NBViewer takes a notebook from a GitHub repository and renders it in a nice, readable way. However, this is a static view of the notebook. If you want to run the code or edit the notebook, you need to run it on Binder. Either click one of the buttons described below or the Binder icon in NBViewer's top menu bar.","title":"Viewing notebooks on NBViewer"},{"location":"getting-started/#running-notebooks-live-on-binder","text":"Each collection of notebooks includes a button. When you click on the button, the Binder service opens the notebooks within a customised computing environment. This can take a little while \u2014 just be patient. Once Binder is ready, you'll be able to use all the notebooks live within your web browser. However, if you make any changes or download any data, Binder won't save them for you. You'll have to make sure you download any files you want to keep. In many cases the notebooks themselves will generate download links to make it easy for you to save your results. Binder sessions will also stop responding after after a period of inactivity \u2014 just start a new session.","title":"Running notebooks live on Binder"},{"location":"getting-started/#running-notebooks-in-app-mode","text":"Some of the workbench repositories make use of the appmode extension for Jupyter notebooks. When you open a notebook in app mode, all the code cells are hidden and are run automatically as the notebook loads. This means you can make a notebook available with a nice clean interface for those who might be a little intimidated by a page full of code. But the code is still there. To view the underlying notebook, just click on the 'Edit App' button at the top of the page. There are two ways to open a notebook in app mode. If you're in the normal notebook view you should see an appmode button in the menu bar, just click it. To make things easier, I've included buttons under each app \u2014 when you click these buttons, the notebooks open on Binder in app mode. No extra clicks required.","title":"Running notebooks in &lsquo;app mode&rsquo;"},{"location":"getting-started/#running-notebooks-on-your-own-computer","text":"If you have Jupyter running on your own computer you can just clone, fork, or download the repository from GitHub. The link to the repository on GitHub is in the top navigation bar. Once you have a local copy (preferably running in a virtual environment), you can install the siftware you need using the requirements.txt file. ~~More detail required~~","title":"Running notebooks on your own computer"},{"location":"glam-data-portals/","text":"GLAM data from government portals \u00b6 This is an attempt to assemble some useful information about Australian GLAM (Galleries, Libraries, Archives, Museums) datasets. As a first step, I've harvested GLAM-related datasets from the various national and state data portals. I did this by identifying relevant organisations and groups, and then harvesting all the packages associated with them. I also added in a few extra packages that looked relevant. Tools, tips, and examples \u00b6 Harvesting GLAM data from government portals This notebook attempts to harvest the details of GLAM datasets from state and national data portals. It also attempts some analysis of the results. Results (April 2018) \u00b6 Here's a CSV containing details of all the datasets I found. I've also uploaded it to Google Sheets. There are duplicates in the data because some datasets are listed on more than one portal. While my interest is in datasets containing collection data, the list also includes datasets created by the operations of GLAM organisations, such as borrowing data or FOI reports. I might filter these out later on. There are currently 790 datasets in this list. Here's the number of datasets by data portal: data.gov.au 271 data.qld.gov.au 214 data.sa.gov.au 173 data.wa.gov.au 96 data.nsw.gov.au 30 data.vic.gov.au 6 And the number of datasets by organisation: State Library of South Australia 121 Housing and Public Works 117 State Library of Western Australia 114 Natural Resources, Mines and Energy 79 State Library of Queensland 78 LINC Tasmania 74 State Records 41 State Records Office of Western Australia 41 South Australian Governments 26 State Library of New South Wales 21 State Archives NSW 19 Environment and Science 14 History Trust of South Australia 12 State Library of NSW 6 State Library of Victoria 6 National Library of Australia 5 Aboriginal and Torres Strait Islander Partnerships 4 Museum of Applied Arts and Sciences 3 National Archives of Australia 3 National Portrait Gallery 2 Mount Gambier Library 2 Australian Museum 1 City of Sydney 1 I've attempted to identify the format of each dataset by checking the file extension. If there's no file extension I use the format value in the package metadata. These values don't always seem reliable. Here's the number of datasets by format: csv 499 xml 66 wms 35 xlsx 27 json 25 docx 17 xls 16 txt 15 zip 14 doc 12 api 12 geojson 8 other 7 data 6 pdf 4 jpg 2 html 2 rss 2 website link 2 kml 2 rtf 2 kmz 1 css, java, php, javascript 1 php 1 xsd 1 csv, json, web services 1 mp3 1 js 1 museum 1 website 1 app 1 jpeg 1 url 1 .txt 1 wfs 1 plain 1 For each dataset, I've fired off a HEAD request for the url to see if the link still works. Here's the number of datasets by HTTP status code ( 200 is ok, 404 is not found): 200 746 404 39 400 3 403 2 Just the CSVs \u00b6 I've created a CSV of just the CSV-formatted datasets . I've also uploaded it to Google Sheets. There are 499 CSV-formatted datasets in this list. Here are results of the HEAD requests for CSV-formatted datasets: 200 493 404 4 400 2","title":"Gov data portals"},{"location":"glam-data-portals/#glam-data-from-government-portals","text":"This is an attempt to assemble some useful information about Australian GLAM (Galleries, Libraries, Archives, Museums) datasets. As a first step, I've harvested GLAM-related datasets from the various national and state data portals. I did this by identifying relevant organisations and groups, and then harvesting all the packages associated with them. I also added in a few extra packages that looked relevant.","title":"GLAM data from government portals"},{"location":"glam-data-portals/#tools-tips-and-examples","text":"Harvesting GLAM data from government portals This notebook attempts to harvest the details of GLAM datasets from state and national data portals. It also attempts some analysis of the results.","title":"Tools, tips, and examples"},{"location":"glam-data-portals/#results-april-2018","text":"Here's a CSV containing details of all the datasets I found. I've also uploaded it to Google Sheets. There are duplicates in the data because some datasets are listed on more than one portal. While my interest is in datasets containing collection data, the list also includes datasets created by the operations of GLAM organisations, such as borrowing data or FOI reports. I might filter these out later on. There are currently 790 datasets in this list. Here's the number of datasets by data portal: data.gov.au 271 data.qld.gov.au 214 data.sa.gov.au 173 data.wa.gov.au 96 data.nsw.gov.au 30 data.vic.gov.au 6 And the number of datasets by organisation: State Library of South Australia 121 Housing and Public Works 117 State Library of Western Australia 114 Natural Resources, Mines and Energy 79 State Library of Queensland 78 LINC Tasmania 74 State Records 41 State Records Office of Western Australia 41 South Australian Governments 26 State Library of New South Wales 21 State Archives NSW 19 Environment and Science 14 History Trust of South Australia 12 State Library of NSW 6 State Library of Victoria 6 National Library of Australia 5 Aboriginal and Torres Strait Islander Partnerships 4 Museum of Applied Arts and Sciences 3 National Archives of Australia 3 National Portrait Gallery 2 Mount Gambier Library 2 Australian Museum 1 City of Sydney 1 I've attempted to identify the format of each dataset by checking the file extension. If there's no file extension I use the format value in the package metadata. These values don't always seem reliable. Here's the number of datasets by format: csv 499 xml 66 wms 35 xlsx 27 json 25 docx 17 xls 16 txt 15 zip 14 doc 12 api 12 geojson 8 other 7 data 6 pdf 4 jpg 2 html 2 rss 2 website link 2 kml 2 rtf 2 kmz 1 css, java, php, javascript 1 php 1 xsd 1 csv, json, web services 1 mp3 1 js 1 museum 1 website 1 app 1 jpeg 1 url 1 .txt 1 wfs 1 plain 1 For each dataset, I've fired off a HEAD request for the url to see if the link still works. Here's the number of datasets by HTTP status code ( 200 is ok, 404 is not found): 200 746 404 39 400 3 403 2","title":"Results (April 2018)"},{"location":"glam-data-portals/#just-the-csvs","text":"I've created a CSV of just the CSV-formatted datasets . I've also uploaded it to Google Sheets. There are 499 CSV-formatted datasets in this list. Here are results of the HEAD requests for CSV-formatted datasets: 200 493 404 4 400 2","title":"Just the CSVs"},{"location":"recordsearch/","text":"RecordSearch \u00b6 RecordSearch is the online collection database of the National Archives of Australia. Based on the series system , RecordSearch provides rich, contextual information about series, items, agencies, and functions. Unfortunately RecordSearch doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. The notebooks here all make use of the RecordSearch Tools library to handle the scraping. Tools, tips, and examples \u00b6 Harvesting a series Harvest details of all items in a series and download images from any digitised files. Series with more than 20,000 items can be a bit tricky, but some strategies for dealing with these are included as well. Harvest files with the access status of 'closed' The National Archives of Australia's RecordSearch database includes some information about files that we're not allowed to see. These files have been through the access examination process and ended up with an access status of 'closed'. While you can search by access status in RecordSearch, you can't explore the reasons, so if you want to dig any deeper you need to harvest the data. This notebook shows you how. Harvesting functions from the RecordSearch interface This notebook attempts to extract information from the RecordSearch interface about the hierarchy of functions it uses to describe the work of government agencies. Previous explorations have shown that the NAA's use of functions is rather inconsistent. All I'm doing here is finding out what functions RecordSearch itself says it is using. This may not be complete, but it seems like a useful starting point. How many of the functions are actually used? In this notebook we'll import data about functions that we've harvested earlier and search for each of these functions in RecordSearch to see how many are actually used. Harvest agencies associated with all functions This notebook loops through the list of functions that were extracted from the RecordSearch interface and saves basic details of the agencies responsible for each function. To keep down the file size and avoid too much duplication it doesn't include the full range of relationships that an agency might have. If you want the full agency data, use the app below to harvest agencies associated with an individual function or hierarchy. Useful apps \u00b6 These are Jupyter notebooks designed to run in 'app mode' with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the 'Edit App' button. Download the contents of a digitised file RecordSearch lets you download a PDF of a digitised file, but sometimes it's more convenient to work with individual images. Just give this app the barcode of a digitised file and it will grab all the images as JPGs, zip them up into a folder, and generate a download link. Get a list of agencies associated with a function RecordSearch describes the business of government in terms of 'functions'. A function is an area of responsibility assigned to a particular government agency. Over time, functions change and move between agencies. If you're wanting to track particular areas of government activity, such as 'migration' or 'meteorology', it can be useful to start with functions, then follow the trail through agencies, series created by those agencies, and finally items contained within those series. This app makes it easy for you to download a list agencies associated with a particular function. Who's responsible The National Archives of Australia's RecordSearch database divides government activities up into a series of functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from RecordSearch to create a a simple visualisation of the agencies responsible for a selected function.","title":"RecordSearch"},{"location":"recordsearch/#recordsearch","text":"RecordSearch is the online collection database of the National Archives of Australia. Based on the series system , RecordSearch provides rich, contextual information about series, items, agencies, and functions. Unfortunately RecordSearch doesn't provide access to machine-readable data through an API, so we have to resort to screen scraping. The notebooks here all make use of the RecordSearch Tools library to handle the scraping.","title":"RecordSearch"},{"location":"recordsearch/#tools-tips-and-examples","text":"Harvesting a series Harvest details of all items in a series and download images from any digitised files. Series with more than 20,000 items can be a bit tricky, but some strategies for dealing with these are included as well. Harvest files with the access status of 'closed' The National Archives of Australia's RecordSearch database includes some information about files that we're not allowed to see. These files have been through the access examination process and ended up with an access status of 'closed'. While you can search by access status in RecordSearch, you can't explore the reasons, so if you want to dig any deeper you need to harvest the data. This notebook shows you how. Harvesting functions from the RecordSearch interface This notebook attempts to extract information from the RecordSearch interface about the hierarchy of functions it uses to describe the work of government agencies. Previous explorations have shown that the NAA's use of functions is rather inconsistent. All I'm doing here is finding out what functions RecordSearch itself says it is using. This may not be complete, but it seems like a useful starting point. How many of the functions are actually used? In this notebook we'll import data about functions that we've harvested earlier and search for each of these functions in RecordSearch to see how many are actually used. Harvest agencies associated with all functions This notebook loops through the list of functions that were extracted from the RecordSearch interface and saves basic details of the agencies responsible for each function. To keep down the file size and avoid too much duplication it doesn't include the full range of relationships that an agency might have. If you want the full agency data, use the app below to harvest agencies associated with an individual function or hierarchy.","title":"Tools, tips, and examples"},{"location":"recordsearch/#useful-apps","text":"These are Jupyter notebooks designed to run in 'app mode' with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the 'Edit App' button. Download the contents of a digitised file RecordSearch lets you download a PDF of a digitised file, but sometimes it's more convenient to work with individual images. Just give this app the barcode of a digitised file and it will grab all the images as JPGs, zip them up into a folder, and generate a download link. Get a list of agencies associated with a function RecordSearch describes the business of government in terms of 'functions'. A function is an area of responsibility assigned to a particular government agency. Over time, functions change and move between agencies. If you're wanting to track particular areas of government activity, such as 'migration' or 'meteorology', it can be useful to start with functions, then follow the trail through agencies, series created by those agencies, and finally items contained within those series. This app makes it easy for you to download a list agencies associated with a particular function. Who's responsible The National Archives of Australia's RecordSearch database divides government activities up into a series of functions. Over time, different agencies have been made responsible for these functions, and it can be interesting to track how these responsibilities have shifted. This notebook uses data about functions harvested from RecordSearch to create a a simple visualisation of the agencies responsible for a selected function.","title":"Useful apps"},{"location":"tepapa/","text":"Te Papa collections API \u00b6 Te Papa has a very well-documented API that provides rich information about its collection and the relationships between collection items and other entities, including people and places. You'll need an API key for serious exploration. Tips, tools, and examples \u00b6 Exploring the Te Papa collection API This notebook is just a preliminary exploration of the API. It drills down through some of the facets to try and get a picture of what data is available. Mapping Te Papa's collections This notebook creates some simple maps using the production.spatial facet of the Te Papa API to identify places where collection objects were created.","title":"Te Papa"},{"location":"tepapa/#te-papa-collections-api","text":"Te Papa has a very well-documented API that provides rich information about its collection and the relationships between collection items and other entities, including people and places. You'll need an API key for serious exploration.","title":"Te Papa collections API"},{"location":"tepapa/#tips-tools-and-examples","text":"Exploring the Te Papa collection API This notebook is just a preliminary exploration of the API. It drills down through some of the facets to try and get a picture of what data is available. Mapping Te Papa's collections This notebook creates some simple maps using the production.spatial facet of the Te Papa API to identify places where collection objects were created.","title":"Tips, tools, and examples"},{"location":"trove-harvester/","text":"Download large quantities of digitised newspaper articles from Trove using the Trove Harvester tool. Tools, tips, and examples \u00b6 Using TroveHarvester to get newspaper articles in bulk An easy introduction to the Trove Harvester command line tool. Edit a few cells and you'll be harvesting metadata and full text of thousands of newspaper articles in minutes. Exploring your TroveHarvester data This notebook shows some ways in which you can analyse and visualise the article metadata you've harvested \u2014 show the distribution of articles over time and space; find which newspapers published the most articles. (Under construction) Exploring harvested text files This notebook suggests some ways in which you can aggregate and analyse the individual OCRd text files for each article \u2014 look at word frequencies ; calculate TF-IDF values. (Under construction) Useful apps \u00b6 These are Jupyter notebooks designed to run in \u2018app mode\u2019 with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the \u2018Edit App\u2019 button. Trove Harvester web app A simple web interface to the TroveHarvester, the easiest way to harvest data from Trove.","title":"Trove newspaper harvester"},{"location":"trove-harvester/#tools-tips-and-examples","text":"Using TroveHarvester to get newspaper articles in bulk An easy introduction to the Trove Harvester command line tool. Edit a few cells and you'll be harvesting metadata and full text of thousands of newspaper articles in minutes. Exploring your TroveHarvester data This notebook shows some ways in which you can analyse and visualise the article metadata you've harvested \u2014 show the distribution of articles over time and space; find which newspapers published the most articles. (Under construction) Exploring harvested text files This notebook suggests some ways in which you can aggregate and analyse the individual OCRd text files for each article \u2014 look at word frequencies ; calculate TF-IDF values. (Under construction)","title":"Tools, tips, and examples"},{"location":"trove-harvester/#useful-apps","text":"These are Jupyter notebooks designed to run in \u2018app mode\u2019 with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the \u2018Edit App\u2019 button. Trove Harvester web app A simple web interface to the TroveHarvester, the easiest way to harvest data from Trove.","title":"Useful apps"},{"location":"trove-lists/","text":"Trove lists are user created collections of items. The details of public lists are available through the Trove API. Tips, tools, and examples \u00b6 Harvest summary data from Trove lists Use the Trove API to harvest data about all public lists, then extract some summary data and explore a few different techniques to analyse the complete dataset. Convert a Trove list into a CSV file Use the Trove API to save the contents of a public list to a CSV file.","title":"Trove lists"},{"location":"trove-lists/#tips-tools-and-examples","text":"Harvest summary data from Trove lists Use the Trove API to harvest data about all public lists, then extract some summary data and explore a few different techniques to analyse the complete dataset. Convert a Trove list into a CSV file Use the Trove API to save the contents of a public list to a CSV file.","title":"Tips, tools, and examples"},{"location":"trove-newspapers/","text":"Assorted experiments and examples working with Trove\u2019s digitised newspapers Tips, tools, and examples \u00b6 Map Trove newspaper results by state Uses the Trove state facet to create a choropleth map that visualises the number of search results per state. Map Trove newspaper results by place of publication Uses the Trove title facet to find the number of results per newspaper, then merges the results with a dataset of geolocated newspapers to map where articles were published. Map Trove newspaper results by place of publication over time Adds a time dimension to the examples in the previous notebook to create an animated heatmap. Today\u2019s news yesterday Uses the date index and the firstpageseq parameter to find articles from exactly 100 years ago that were published on the front page. It then selects one of the articles at random and downloads and displays an image of the front page. Create a Trove OCR corrections ticker Uses the has:corrections parameter to get the total number of newspaper articles with OCR corrections, then displays the results, updating every five seconds. Useful apps \u00b6 These are Jupyter notebooks designed to run in \u2018app mode\u2019 with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the \u2018Edit App\u2019 button. Download a page image The Trove web interface doesn\u2019t provide a way of getting high-resolution page images from newspapers. This simple app lets you download page images as complete, high-resolution JPG files. QueryPic Deconstructed QueryPic is a tool I created many years ago to visualise searches in Trove's digitised newspapers. It shows you the number of articles each year that match your query \u2014 instead of a page of search results, you see the complete result set. You can look for patterns and trends across time. This is a deconstructed, extended, and hackable version of QueryPic.","title":"Trove newspapers"},{"location":"trove-newspapers/#tips-tools-and-examples","text":"Map Trove newspaper results by state Uses the Trove state facet to create a choropleth map that visualises the number of search results per state. Map Trove newspaper results by place of publication Uses the Trove title facet to find the number of results per newspaper, then merges the results with a dataset of geolocated newspapers to map where articles were published. Map Trove newspaper results by place of publication over time Adds a time dimension to the examples in the previous notebook to create an animated heatmap. Today\u2019s news yesterday Uses the date index and the firstpageseq parameter to find articles from exactly 100 years ago that were published on the front page. It then selects one of the articles at random and downloads and displays an image of the front page. Create a Trove OCR corrections ticker Uses the has:corrections parameter to get the total number of newspaper articles with OCR corrections, then displays the results, updating every five seconds.","title":"Tips, tools, and examples"},{"location":"trove-newspapers/#useful-apps","text":"These are Jupyter notebooks designed to run in \u2018app mode\u2019 with the code cells hidden. The Binder buttons will automatically open the notebooks in app mode, but you can always view and edit the code by clicking the \u2018Edit App\u2019 button. Download a page image The Trove web interface doesn\u2019t provide a way of getting high-resolution page images from newspapers. This simple app lets you download page images as complete, high-resolution JPG files. QueryPic Deconstructed QueryPic is a tool I created many years ago to visualise searches in Trove's digitised newspapers. It shows you the number of articles each year that match your query \u2014 instead of a page of search results, you see the complete result set. You can look for patterns and trends across time. This is a deconstructed, extended, and hackable version of QueryPic.","title":"Useful apps"},{"location":"trove/","text":"Trove provides access to much of it's data through an API (Application Programming Interface). The notebooks in this section provide many examples of using the API to harvest data and analyse the contents of Trove. Before you can use the API you need to obtain a key \u2014 it's free and quick. Just follow these instructions . Useful links \u00b6 Trove API Documentation Trove API console","title":"Trove API introduction"},{"location":"trove/#useful-links","text":"Trove API Documentation Trove API console","title":"Useful links"}]}